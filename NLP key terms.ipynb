{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04cf29e",
   "metadata": {},
   "source": [
    "### 1. Tokenization\n",
    "**Definition**:  \n",
    "Tokenization is the process of splitting text into individual units or tokens, such as words, sentences, or subwords.\n",
    "\n",
    "**Types of Tokenization**:\n",
    "- **Word Tokenization**: Splitting text into words.\n",
    "- **Sentence Tokenization**: Splitting text into sentences.\n",
    "\n",
    "**Applications**:\n",
    "- Preprocessing step in NLP tasks like translation, summarization, and classification.\n",
    "\n",
    "**Example**:  \n",
    "Text: \"I love Python!\"  \n",
    "Tokens: `[\"I\", \"love\", \"Python\", \"!\"]`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Stemming\n",
    "**Definition**:  \n",
    "Stemming is the process of reducing a word to its root form by removing prefixes or suffixes.\n",
    "\n",
    "**Goal**:  \n",
    "To normalize different variations of a word into a base form.\n",
    "\n",
    "**Example**:  \n",
    "- \"running\" → \"run\"  \n",
    "- \"happiness\" → \"happi\"  \n",
    "\n",
    "**Algorithms**:\n",
    "- **Porter Stemmer**\n",
    "- **Lancaster Stemmer**\n",
    "\n",
    "**Applications**:\n",
    "- Used in information retrieval to group words with the same root.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Lemmatization\n",
    "**Definition**:  \n",
    "Lemmatization is the process of reducing a word to its base or dictionary form (lemma), considering context and meaning.\n",
    "\n",
    "**Difference from Stemming**:  \n",
    "- Stemming only removes affixes, while lemmatization transforms a word into its meaningful root form.\n",
    "\n",
    "**Example**:  \n",
    "- \"better\" → \"good\"  \n",
    "- \"running\" → \"run\"\n",
    "\n",
    "**Applications**:\n",
    "- More accurate than stemming for tasks requiring semantics (e.g., text classification, sentiment analysis).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Bag of Words (BoW)\n",
    "**Definition**:  \n",
    "BoW is a model used to represent text data in which the order of words is ignored, and only their frequencies are considered.\n",
    "\n",
    "**How it Works**:\n",
    "- Each word in the corpus is treated as a feature.\n",
    "- The text is represented by a vector of word counts or occurrences.\n",
    "\n",
    "**Example**:\n",
    "Text: \"I love programming and I love Python\"\n",
    "Bag of Words Representation:  \n",
    "`{\"I\": 2, \"love\": 2, \"programming\": 1, \"and\": 1, \"Python\": 1}`\n",
    "\n",
    "**Applications**:\n",
    "- Text classification, document clustering, sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "**Definition**:  \n",
    "TF-IDF is a statistic used to measure the importance of a word in a document relative to its frequency across the corpus.\n",
    "\n",
    "**Formula**:\n",
    "- **TF** (Term Frequency): The frequency of a term in a document.\n",
    "  $\n",
    "  TF(t, d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}}\n",
    "  $\n",
    "- **IDF** (Inverse Document Frequency): The inverse of the number of documents containing the term.\n",
    "  $\n",
    "  IDF(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term t}}\\right)\n",
    "  $\n",
    "- **TF-IDF**: The product of TF and IDF.\n",
    "  $\n",
    "  TF\\_IDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "  $\n",
    "\n",
    "**Applications**:\n",
    "- Text search engines, document classification, information retrieval.\n",
    "\n",
    "**Example**:  \n",
    "For a word \"Python\" in a document with 100 terms and appearing 3 times:  \n",
    "TF = 3/100, IDF = log(100/10)  \n",
    "TF-IDF would measure its relevance based on both frequency and rarity.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Word Embedding (Word2Vec)\n",
    "**Definition**:  \n",
    "Word2Vec is a deep learning-based technique for transforming words into continuous vector representations, capturing semantic relationships between words.\n",
    "\n",
    "**How it Works**:\n",
    "- **Skip-Gram**: Predicts the context words given a target word.\n",
    "- **Continuous Bag of Words (CBOW)**: Predicts the target word from context words.\n",
    "\n",
    "**Benefits**:\n",
    "- Captures semantic meanings and word similarities (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\").\n",
    "- Reduces dimensionality of word representations.\n",
    "\n",
    "**Example**:  \n",
    "Word2Vec can map words like \"king\" and \"queen\" close to each other in vector space.\n",
    "\n",
    "**Applications**:\n",
    "- Text classification, sentiment analysis, recommendation systems.\n",
    "\n",
    "**Libraries**:  \n",
    "- **Gensim**: Popular library for training Word2Vec models.\n",
    "\n",
    "\n",
    "### 7. Named Entity Recognition (NER) - Short Notes\n",
    "\n",
    "**Definition**:  \n",
    "NER is a natural language processing (NLP) task that identifies and classifies proper nouns or entities in text into predefined categories such as people, organizations, locations, dates, etc.\n",
    "\n",
    "**Key Categories**:\n",
    "- **Person (PER)**: Names of people (e.g., \"Elon Musk\")\n",
    "- **Location (LOC)**: Geographical locations (e.g., \"Paris\")\n",
    "- **Organization (ORG)**: Companies or institutions (e.g., \"Google\")\n",
    "- **Date (DATE)**: Temporal expressions (e.g., \"January 1, 2024\")\n",
    "- **Other**: Time, Money, Percentage, etc.\n",
    "\n",
    "**NER Process**:\n",
    "1. **Tokenization**: Breaking the text into tokens (words).\n",
    "2. **Entity Detection**: Identifying potential named entities.\n",
    "3. **Entity Classification**: Categorizing the identified entities into types.\n",
    "\n",
    "**Applications**:\n",
    "- Information extraction\n",
    "- Question answering\n",
    "- Search engine optimization\n",
    "- Content categorization\n",
    "\n",
    "**Example**:  \n",
    "Text: \"Elon Musk, the CEO of SpaceX, visited New York on December 25, 2023.\"  \n",
    "Output:  \n",
    "- \"Elon Musk\" → Person (PER)  \n",
    "- \"SpaceX\" → Organization (ORG)  \n",
    "- \"New York\" → Location (LOC)  \n",
    "- \"December 25, 2023\" → Date (DATE)\n",
    "\n",
    "**Challenges**:  \n",
    "- Ambiguity of terms (e.g., \"Apple\" could be a company or fruit)\n",
    "- Domain-specific entities (e.g., medical terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab17714",
   "metadata": {},
   "source": [
    "## **8. Normalization**\n",
    "- **Definition**: Standardizing text for uniformity.\n",
    "- **Tasks**:\n",
    "  - Convert to lowercase: \"Text\" → \"text\".\n",
    "  - Remove punctuation: \"Hello, world!\" → \"Hello world\".\n",
    "  - Expand contractions: \"don't\" → \"do not\".\n",
    "- **Purpose**: Ensures consistent text representation.\n",
    "\n",
    "## **9. Corpus**\n",
    "- **Definition**: A collection of texts for linguistic or statistical analysis.\n",
    "- **Types**:\n",
    "  - Monolingual: Texts in one language.\n",
    "  - Multilingual: Texts across multiple languages.\n",
    "- **Example**: Historical or scientific texts.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Stop Words**\n",
    "- **Definition**: Common words (e.g., \"the,\" \"is\") removed from text to focus on meaningful content.\n",
    "- **Example**:\n",
    "  Original: \"The quick brown fox jumps over the lazy dog.\"  \n",
    "  Without Stop Words: \"quick brown fox jumps lazy dog\"\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Parts-of-Speech (POS) Tagging**\n",
    "- **Definition**: Assigning grammatical tags to words (e.g., noun, verb, adjective).\n",
    "- **Purpose**: Helps in understanding the structure and meaning of text.\n",
    "- **Example**:\n",
    "  - \"The cat sleeps.\"\n",
    "    - \"The\" → Determiner\n",
    "    - \"cat\" → Noun\n",
    "    - \"sleeps\" → Verb\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Statistical Language Modeling**\n",
    "- **Definition**: Estimating probabilities of word sequences to predict the likelihood of a text.\n",
    "- **Applications**:\n",
    "  - Speech recognition\n",
    "  - Text generation\n",
    "- **Example**: Predicting the next word in \"The weather is...\"\n",
    "\n",
    "---\n",
    "\n",
    "## **13. n-grams**\n",
    "\n",
    "- **Definition**: A text representation model that preserves contiguous sequences of N items (words or characters) from a text selection.\n",
    "- **Difference from Bag of Words**: Unlike Bag of Words, n-grams consider word order within a sequence.\n",
    "- **Example**: Trigrams (3-grams) for the sentence:  \n",
    "  \"There, there,\" said James. \"There, there.\"  \n",
    "  Representation:\n",
    "  ```python\n",
    "  [\n",
    "      \"there there said\",\n",
    "      \"there said james\",\n",
    "      \"said james there\",\n",
    "      \"james there there\"\n",
    "  ]\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Regular Expressions**\n",
    "\n",
    "- **Definition**: Regular expressions (regex or regexp) are concise patterns used for searching, matching, and manipulating text.\n",
    "- **Purpose**: Extend beyond simple wildcard characters (`*`, `?`) to define complex text patterns.\n",
    "- **Example Use Case**: Extracting email addresses from text:\n",
    "  ```python\n",
    "  import re\n",
    "  \n",
    "  text = \"Contact us at support@example.com or sales@example.org\"\n",
    "  pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "  emails = re.findall(pattern, text)\n",
    "  print(emails)  # Output: ['support@example.com', 'sales@example.org']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c41fb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences: ['I love programming.', 'Python is my favorite language!', 'I am learning Natural Language Processing.']\n",
      "Tokenized Words: ['I', 'love', 'programming', '.', 'Python', 'is', 'my', 'favorite', 'language', '!', 'I', 'am', 'learning', 'Natural', 'Language', 'Processing', '.']\n",
      "\n",
      "Stemmed Words: ['i', 'love', 'program', 'python', 'is', 'my', 'favorit', 'languag', 'i', 'am', 'learn', 'natur', 'languag', 'process']\n",
      "\n",
      "Lemmatized Words: ['I', 'love', 'programming', 'Python', 'is', 'my', 'favorite', 'language', 'I', 'am', 'learning', 'Natural', 'Language', 'Processing']\n",
      "\n",
      "Bag of Words Representation: [[1 1 1 2 1 1 1 1 1 1 1]]\n",
      "\n",
      "TF-IDF Representation: [[0.26726124 0.26726124 0.26726124 0.53452248 0.26726124 0.26726124\n",
      "  0.26726124 0.26726124 0.26726124 0.26726124 0.26726124]]\n",
      "\n",
      "Word2Vec Vector for 'love': [ 1.62645429e-02 -8.91466811e-03 -2.13671452e-03  2.01272964e-03\n",
      " -3.82227910e-04  2.29635485e-03  1.22277215e-02 -4.05430801e-05\n",
      " -6.49193069e-03 -3.02145723e-03  1.17945978e-02  3.02820443e-03\n",
      " -1.44852395e-03  1.86664946e-02 -9.84256715e-03 -1.67681929e-03\n",
      "  1.83508229e-02  1.34988548e-02  3.00571206e-03 -1.77651215e-02\n",
      "  2.29749200e-03 -4.57651122e-03  1.87364742e-02  2.41985568e-03\n",
      "  2.98012723e-03  4.81281988e-03 -3.67201329e-03 -9.99926776e-03\n",
      "  4.64859011e-04 -4.02836083e-03  1.32018663e-02  1.78802460e-02\n",
      " -1.34950876e-03  5.95402950e-03 -1.22153088e-02  3.39864963e-03\n",
      " -1.38524650e-02 -1.73880532e-02 -1.18004056e-02 -1.79129504e-02\n",
      "  1.45551898e-02 -1.15440628e-02  1.65527035e-02 -1.44870905e-02\n",
      "  6.84334990e-03  1.93499979e-02 -1.55708957e-02 -1.98901147e-02\n",
      " -8.65829270e-03 -5.36626112e-03]\n",
      "\n",
      "Most Similar Words to 'love': [('Natural', 0.19010192155838013), ('programming', 0.17440013587474823), ('favorite', 0.11519220471382141), ('am', 0.10159842669963837), ('Processing', 0.08061248809099197), ('I', 0.04067763686180115), ('learning', -0.023305965587496758), ('language', -0.029589535668492317), ('Language', -0.03339873254299164), ('my', -0.06483978033065796)]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "\n",
    "# Sample text\n",
    "text = \"I love programming. Python is my favorite language! I am learning Natural Language Processing.\"\n",
    "\n",
    "# 1. Tokenization\n",
    "sentences = sent_tokenize(text)  # Sentence Tokenization\n",
    "words = word_tokenize(text)      # Word Tokenization\n",
    "print(\"Tokenized Sentences:\", sentences)\n",
    "print(\"Tokenized Words:\", words)\n",
    "\n",
    "# 2. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words if word not in string.punctuation]\n",
    "print(\"\\nStemmed Words:\", stemmed_words)\n",
    "\n",
    "# 3. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words if word not in string.punctuation]\n",
    "print(\"\\nLemmatized Words:\", lemmatized_words)\n",
    "\n",
    "# 4. Bag of Words (BoW)\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform([text])  # Fit and transform the text\n",
    "print(\"\\nBag of Words Representation:\", bow_matrix.toarray())\n",
    "\n",
    "# 5. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "print(\"\\nTF-IDF Representation:\", tfidf_matrix.toarray())\n",
    "\n",
    "# 6. Word Embedding (Word2Vec)\n",
    "# Create a list of sentences for Word2Vec\n",
    "sentences_list = [[\"I\", \"love\", \"programming\"], [\"Python\", \"is\", \"my\", \"favorite\", \"language\"], \n",
    "                  [\"I\", \"am\", \"learning\", \"Natural\", \"Language\", \"Processing\"]]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences_list, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get vector for the word 'love'\n",
    "vector = model.wv[\"love\"]\n",
    "print(\"\\nWord2Vec Vector for 'love':\", vector)\n",
    "\n",
    "# Find most similar words to 'love'\n",
    "similar_words = model.wv.most_similar(\"love\")\n",
    "print(\"\\nMost Similar Words to 'love':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f6d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f32782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
